
@book{bishop2023deep,
  title={Deep learning: Foundations and concepts},
  author={Bishop, Christopher M and Bishop, Hugh},
  year={2023},
  publisher={Springer Nature}
}

@inproceedings{wu2024hector,
  title={Hector: An efficient programming and compilation framework for implementing relational graph neural networks in GPU architectures},
  author={Wu, Kun and Hidayeto{\u{g}}lu, Mert and Song, Xiang and Huang, Sitao and Zheng, Da and Nisa, Israt and Hwu, Wen-mei},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={528--544},
  year={2024}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@inproceedings{lattner2021mlir,
  title={MLIR: Scaling compiler infrastructure for domain specific computation},
  author={Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={2--14},
  year={2021},
  organization={IEEE}
}

@article{li2020deep,
  title={The deep learning compiler: A comprehensive survey},
  author={Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={32},
  number={3},
  pages={708--727},
  year={2020},
  publisher={IEEE}
}

@article{jia2019,
  title={Optimizing DNN computation with relaxed graph substitutions},
  author={Jia, Zhihao and Thomas, James and Warszawski, Todd and Gao, Mingyu and Zaharia, Matei and Aiken, Alex},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={27--39},
  year={2019}
}

@inproceedings{he2016,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{cormen2009introduction,
  title={Introduction to Algorithms. Thrid Edition},
  author={Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
  journal={London: Massachusetts},
  year={2009}
}

@manual{hloir,
	author= {TensorFlow},
    title={HLO IR, https://github.com/tensorflow/mlir-hlo},
	url={https://github.com/tensorflow/mlir-hlo}
	year={2023}
}


@manual{halideir,
	author= {TVM project},
    title={Halide IR, https://github.com/dmlc/HalideIR},
	url={https://github.com/dmlc/HalideIR}
	year={2023}
}

@manual{pytorchdocs,
	author = {The Linux Foundation},
	title = {PyTorch Docs, https://pytorch.org/docs/stable/index.html},
	url = {https://pytorch.org/docs/stable/index.html},
	year = {2023}
}

@manual{onnxpython,
	author= {The Linux Foundation},
  title        = {ONNX With Python, https://onnx.ai/onnx/intro/python.html},
  url = {https://onnx.ai/onnx/intro/python.html},
  year={2024}

}

@manual{onnxoptimizer,
	author= {The Linux Foundation},
  title        = {ONNX Optimizer, https://github.com/onnx/optimizer},
  url = {https://github.com/onnx/optimizer},
	year={2024}
}

@article{click1995,
author = {Click, Cliff and Cooper, Keith D.},
title = {Combining analyses, combining optimizations},
year = {1995},
issue_date = {March 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {0164-0925},
url = {https://doi.org/10.1145/201059.201061},
doi = {10.1145/201059.201061},
abstract = {Modern optimizing compilers use several passes over a program's intermediate representation to generate good code. Many of these optimizations exhibit a phase-ordering problem. Getting the best code may require iterating optimizations until a fixed point is reached. Combining these phases can lead to the discovery of more facts about the program, exposing more opportunities for optimization. This article presents a framework for describing optimizations. It shows how to combine two such frameworks and how to reason about the properties of the resulting framework. The structure of the frame work provides insight into when a combination yields better results. To make the ideas more concrete, this article presents a framework for combining constant propagation, value numbering, and unreachable-code elimination. It is an open question as to what other frameworks can be combined in this way.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {mar},
pages = {181–196},
numpages = {16},
keywords = {constant propagation, data-flow analysis, optimizing compilers, value numbering}
}

  
@inproceedings{karthik2002,
author = {Gargi, Karthik},
title = {A sparse algorithm for predicated global value numbering},
year = {2002},
isbn = {1581134630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512529.512536},
doi = {10.1145/512529.512536},
abstract = {This paper presents a new algorithm for performing global value numbering on a routine in static single assignment form. Our algorithm has all the strengths of the most powerful existing practical methods of global value numbering; it unifies optimistic value numbering with constant folding, algebraic simplification and unreachable code elimination. It goes beyond existing methods by unifying optimistic value numbering with further analyses: it canonicalizes the structure of expressions in order to expose more congruences by performing global reassociation, it exploits the congruences induced by the predicates of conditional jumps (predicate inference and value inference), and it associates the arguments of acyclic \o{} functions with the predicates controlling their arrival (\o{} predication), thus enabling congruence finding on conditional control structures. Finally, it implements an efficient sparse formulation and offers a range of tradeoffs between compilation time and optimization strength. We describe an implementation of the algorithm and present measurements of its strength and efficiency collected when optimizing the SPEC CINT2000 C benchmarks.},
booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design and Implementation},
pages = {45–56},
numpages = {12},
keywords = {static single assignment, global value numbering},
location = {Berlin, Germany},
series = {PLDI '02}
}

  
 @inproceedings{10.1145/2951913.2976746,
author = {Abadi, Mart\'{\i}n},
title = {TensorFlow: learning functions at scale},
year = {2016},
isbn = {9781450342193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2951913.2976746},
doi = {10.1145/2951913.2976746},
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Its computational model is based on dataflow graphs with mutable state. Graph nodes may be mapped to different machines in a cluster, and within each machine to CPUs, GPUs, and other devices. TensorFlow supports a variety of applications, but it particularly targets training and inference with deep neural networks. It serves as a platform for research and for deploying machine learning systems across many areas, such as speech recognition, computer vision, robotics, information retrieval, and natural language processing. In this talk, we describe TensorFlow and outline some of its applications. We also discuss the question of what TensorFlow and deep learning may have to do with functional programming. Although TensorFlow is not purely functional, many of its uses are concerned with optimizing functions (during training), then with applying those functions (during inference). These functions are defined as compositions of simple primitives (as is common in functional programming), with internal data representations that are learned rather than manually designed. TensorFlow is joint work with many other people in the Google Brain team and elsewhere. More information is available at tensorflow.org.},
booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
pages = {1},
numpages = {1},
keywords = {Machine learning, distributed programming},
location = {Nara, Japan},
series = {ICFP 2016}
}

  

@article{tf2016,
author = {Abadi, Mart\'{\i}n},
title = {TensorFlow: learning functions at scale},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/3022670.2976746},
doi = {10.1145/3022670.2976746},
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Its computational model is based on dataflow graphs with mutable state. Graph nodes may be mapped to different machines in a cluster, and within each machine to CPUs, GPUs, and other devices. TensorFlow supports a variety of applications, but it particularly targets training and inference with deep neural networks. It serves as a platform for research and for deploying machine learning systems across many areas, such as speech recognition, computer vision, robotics, information retrieval, and natural language processing. In this talk, we describe TensorFlow and outline some of its applications. We also discuss the question of what TensorFlow and deep learning may have to do with functional programming. Although TensorFlow is not purely functional, many of its uses are concerned with optimizing functions (during training), then with applying those functions (during inference). These functions are defined as compositions of simple primitives (as is common in functional programming), with internal data representations that are learned rather than manually designed. TensorFlow is joint work with many other people in the Google Brain team and elsewhere. More information is available at tensorflow.org.},
journal = {SIGPLAN Not.},
month = {sep},
pages = {1},
numpages = {1},
keywords = {Machine learning, distributed programming}
}

  

 



